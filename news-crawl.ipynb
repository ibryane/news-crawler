{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "\n",
    "class Dataprocession(object):\n",
    "    '''使用textrank4k接口解析中文新闻获取keywords, abstract'''\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.article = ''\n",
    "    def process(self, ):\n",
    "        for i in self.text:\n",
    "            self.article += i.getText() + '\\n'\n",
    "        self.article = self.article.strip()\n",
    "        keywords = []\n",
    "        abstract = []\n",
    "        ##关键词\n",
    "        tr4w = TextRank4Keyword()\n",
    "        tr4w.analyze(text=self.article, lower=True, window=2)\n",
    "        for item in tr4w.get_keywords(4, word_min_len=1):\n",
    "            keywords.append(item.word)\n",
    "        ##摘要\n",
    "        tr4s = TextRank4Sentence()\n",
    "        tr4s.analyze(text=self.article, lower=True, source = 'all_filters')\n",
    "        for item in tr4s.get_key_sentences(num=3):\n",
    "            abstract.append(item.sentence)\n",
    "        return keywords, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.binary import Binary\n",
    "import pickle\n",
    "import zlib\n",
    "\n",
    "class Mongocache:\n",
    "    def __init__(self, client=None):\n",
    "        self.client = MongoClient('localhost', 27017) if client is None else client\n",
    "        self.db = self.client.cache\n",
    "    \n",
    "    def __contains__(self, url):\n",
    "        try:\n",
    "            self[url]\n",
    "        except KeyError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def __getitem__(self, url):\n",
    "        ##获取信息\n",
    "        record = self.db.news.find_one({'_id': url})\n",
    "        if record:\n",
    "            return record['result']\n",
    "        else:\n",
    "            raise KeyError(url + ' does not exist')\n",
    "    def __setitem__(self, url, result):\n",
    "        ##存储信息，由于压缩过程有递归深度错误，没有压缩\n",
    "        record = {'result': result}\n",
    "        self.db.news.update_one({'_id': url}, {'$set': record}, upsert=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "DEFAULT_TIMEOUT = 10\n",
    "\n",
    "class Download(object):\n",
    "    \"\"\"下载内容\"\"\"\n",
    "    def __init__(self, timeout=None, cache=None, max_try=0): \n",
    "        self.cache = cache\n",
    "        self.timeout = timeout\n",
    "        self.cache = cache\n",
    "        self.max_try = max_try\n",
    "        \n",
    "    def __call__(self, url):\n",
    "        result = None\n",
    "        print(url)\n",
    "        try:\n",
    "            result = self.cache[url]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if result is None:\n",
    "            result = self.start(url)\n",
    "            if self.cache:\n",
    "                self.cache[url] = result\n",
    "        return result\n",
    "        \n",
    "    def start(self, url, headers=None, proxy=None,):\n",
    "        #代理,头设置\n",
    "        if proxy:\n",
    "            scheme = proxy.split(':')[0]\n",
    "            proxies = {scheme: proxy}\n",
    "        else:\n",
    "            proxies = None\n",
    "        if headers is None:\n",
    "            headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}\n",
    "        while self.max_try > 0:\n",
    "            try:\n",
    "                res = requests.get(url, headers=headers, proxies=proxies, timeout=self.timeout)\n",
    "            except Exception:\n",
    "                self.max_try -= 1\n",
    "            else:\n",
    "                break\n",
    "        html = res.content\n",
    "        soup = BeautifulSoup(res.content)\n",
    "        ##请求访问解析网页\n",
    "        content = soup.find('div', class_='WYSIWYG articlePage').findAll('p')\n",
    "        for i in content:\n",
    "            if i.getText().startswith('【'):\n",
    "                content.remove(i)\n",
    "        content1 = ''\n",
    "        for i in content:\n",
    "            content1 += str(i) + '\\n'\n",
    "        content1 = content1.strip()\n",
    "        title = soup.find('h1').string\n",
    "        ##获取网页中新闻文本和title\n",
    "        keywords, abstract = Dataprocession(content).process()\n",
    "        ##获取关键词和摘要\n",
    "        \n",
    "        return {'content': content1, 'title': title, 'keywords': keywords, 'abstract': abstract}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "import socket\n",
    "DEFAULT_TIMEOUT = 10\n",
    "DEFAULT_MAXPAGE = 1\n",
    "DEFAULT_DELAY = 6\n",
    "DEFAULT_MAXTRY = 3\n",
    "DEFAULT_MAXTHREAD = 2\n",
    "class Crawler(object):\n",
    "    def __init__(self, start_url, category, timeout=DEFAULT_TIMEOUT, max_threads=DEFAULT_MAXTHREAD, max_try=DEFAULT_MAXTRY, max_page=DEFAULT_MAXPAGE, delay=DEFAULT_DELAY, cache=Mongocache()):\n",
    "        self.start_url = start_url\n",
    "        self.category = category        \n",
    "        self.page = []\n",
    "        self.max_page = max_page\n",
    "        self.timeout = timeout\n",
    "        self.delay = delay\n",
    "        self.links = []\n",
    "        self.max_try = max_try\n",
    "        self.cache = cache\n",
    "        self.max_threads = max_threads\n",
    "        self.a = Download(timeout=self.timeout, cache=self.cache, max_try=self.max_try)\n",
    "        \n",
    "    def parse_first(self, url=None, headers=None, proxy=None,):\n",
    "        ##爬取网页页面url\n",
    "        if not url:\n",
    "            url = self.start_url + self.category\n",
    "        if proxy:\n",
    "            scheme = proxy.split(':')[0]\n",
    "            proxies = {scheme: proxy}\n",
    "        else:\n",
    "            proxies = None\n",
    "        if headers is None:\n",
    "            headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}\n",
    "        \n",
    "        res = requests.get(url, headers=headers, proxies=proxies, timeout=self.timeout)\n",
    "        sleep(self.delay)\n",
    "        self.page.append(url)\n",
    "        html = res.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        next_link = soup.find('div', {'class': 'sideDiv inlineblock text_align_lang_base_2'}).find('a').attrs['href']\n",
    "        print('download %s'%url)\n",
    "        self.max_page -= 1\n",
    "        while self.max_page > 0:\n",
    "            if next_link:\n",
    "                next_link = urljoin(self.start_url, next_link)                \n",
    "                print('we done one')\n",
    "                self.parse_first(url=next_link)\n",
    "\n",
    "    def parse_next(self, headers=None, proxy=None):\n",
    "        ##爬取网页每页的新闻url\n",
    "        self.parse_first()\n",
    "        if proxy:\n",
    "            scheme = proxy.split(':')[0]\n",
    "            proxies = {scheme: proxy}\n",
    "        else:\n",
    "            proxies = None\n",
    "        if headers is None:\n",
    "            headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}\n",
    "        \n",
    "        for i in self.page:\n",
    "            res1 = requests.get(i, headers=headers, proxies=proxies, timeout=self.timeout)\n",
    "            sleep(self.delay)\n",
    "            html1 = res1.content\n",
    "            soup1 = BeautifulSoup(html1)\n",
    "            links1 = soup1.find('div', class_='largeTitle').find_all('article', class_='js-article-item')\n",
    "            for i in links1:\n",
    "                link2 = urljoin(self.start_url, i.find('a').attrs['href'])\n",
    "                self.links.append(link2)\n",
    "        return self.links\n",
    "    \n",
    "    def storage(self,):\n",
    "        ##存储到数据库\n",
    "        self.parse_next()\n",
    "                    \n",
    "        threads = []\n",
    "        while threads or self.links:\n",
    "            for thread in threads:\n",
    "                if not thread.is_alive():\n",
    "                    threads.remove(thread)\n",
    "            while len(threads) < self.max_threads and self.links:\n",
    "                thread = threading.Thread(target=self.process_queue)\n",
    "                thread.setDaemon(True)\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "            sleep(self.delay)\n",
    "    def process_queue(self,):\n",
    "        while True:\n",
    "            try:\n",
    "                url = self.links.pop()\n",
    "            except IndexError:\n",
    "                break\n",
    "            else:\n",
    "                self.a(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://cn.investing.com/news/stock-market-news\n",
      "https://cn.investing.com/news/stock-market-news/article-541449https://cn.investing.com/news/stock-market-news/article-541451\n",
      "\n",
      "https://cn.investing.com/news/stock-market-news/article-541452\n",
      "https://cn.investing.com/news/stock-market-news/article-541453\n",
      "https://cn.investing.com/news/stock-market-news/article-541454\n",
      "https://cn.investing.com/news/stock-market-news/article-541455\n",
      "https://cn.investing.com/news/stock-market-news/article-541456\n",
      "https://cn.investing.com/news/stock-market-news/article-541458\n",
      "https://cn.investing.com/news/stock-market-news/article-541460\n",
      "https://cn.investing.com/news/stock-market-news/article-541464\n",
      "https://cn.investing.com/news/stock-market-news/article-541465\n",
      "https://cn.investing.com/news/stock-market-news/article-541466\n",
      "https://cn.investing.com/news/stock-market-news/article-541467\n",
      "https://cn.investing.com/news/stock-market-news/article-541469\n",
      "https://cn.investing.com/news/stock-market-news/article-541470\n",
      "https://cn.investing.com/news/stock-market-news/article-541471\n",
      "https://cn.investing.com/news/stock-market-news/article-541480\n",
      "https://cn.investing.com/news/stock-market-news/article-541475\n",
      "https://cn.investing.com/news/stock-market-news/article-541476\n",
      "https://cn.investing.com/news/stock-market-news/article-541477\n",
      "https://cn.investing.com/news/stock-market-news/article-541478\n",
      "https://cn.investing.com/news/stock-market-news/article-541479\n",
      "https://cn.investing.com/news/stock-market-news/article-541481\n",
      "https://cn.investing.com/news/stock-market-news/article-541482\n",
      "https://cn.investing.com/news/stock-market-news/article-541483\n",
      "https://cn.investing.com/news/stock-market-news/article-541484\n",
      "https://cn.investing.com/news/stock-market-news/article-541486\n",
      "https://cn.investing.com/news/stock-market-news/article-541487\n",
      "https://cn.investing.com/news/stock-market-news/article-541488\n",
      "https://cn.investing.com/news/stock-market-news/article-541489\n",
      "https://cn.investing.com/news/stock-market-news/article-541490\n",
      "https://cn.investing.com/news/stock-market-news/article-541491\n",
      "https://cn.investing.com/news/stock-market-news/article-541495\n",
      "https://cn.investing.com/news/stock-market-news/article-541498\n",
      "https://cn.investing.com/news/stock-market-news/article-541493\n"
     ]
    }
   ],
   "source": [
    "start_url = 'https://cn.investing.com/news'\n",
    "cate = '/stock-market-news'\n",
    "b = Crawler(start_url, cate)\n",
    "b.storage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
